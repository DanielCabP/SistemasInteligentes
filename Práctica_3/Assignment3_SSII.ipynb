{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97f51abd",
   "metadata": {},
   "source": [
    "<a id=\"inicio\"></a>\n",
    "<img src=\"./figs/barra_uclm_esiiab.png\" alt=\"Banner UCLM - ESIIAB\" align=\"right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0382fd42",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br><br><br>\n",
    "<h1><font color=\"#B30033\" size=5>Intelligent Systems - Course 2021-2022</font></h1>\n",
    "\n",
    "\n",
    "\n",
    "<h1><font color=\"#B30033\" size=5>Assignment 3: Policy learning</font></h1>\n",
    "\n",
    "\n",
    "<br><br>\n",
    "<div style=\"text-align: left\">\n",
    "<font color=\"#4E70BE\" size=3>Lecturers:</font><br>\n",
    "<ul>\n",
    "  <li><font color=\"#4E70BE\" size=3>Juan Carlos Alfaro Jim√©nez</font><br></li>\n",
    "  <li><font color=\"#4E70BE\" size=3>Guillermo Tom√°s Fern√°ndez Mart√≠n</font><br></li>\n",
    "  <li><font color=\"#4E70BE\" size=3>Jos√© Antonio G√°mez Mart√≠n</font><br></li>\n",
    "  <li><font color=\"#4E70BE\" size=3>Ismael Garc√≠a Varea</font><br></li>\n",
    "  <li><font color=\"#4E70BE\" size=3>Luis Gonz√°lez Naharro</font><br></li>\n",
    "  <li><font color=\"#4E70BE\" size=3>Jes√∫s Mart√≠nez G√≥mez</font><br></li>    \n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88884cb5",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this assignment we will study and put into practice the algorithms studied in unit 5 (value/policy iteration and reinforcement lerning) which are able to learn a policy to solve a given problem. We will use a problem based on the same environment that the one used in Assignments 1 and 2. \n",
    "\n",
    "Analyzing and comparing the performance of the algorithms by running them over different instances of the problem is also a goal of this Assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c995568",
   "metadata": {},
   "source": [
    "##¬†Problem description\n",
    "We use the same environment as in previous assignments but now, instead of pieces of garbage, a set of gems is scattered over the maze. Furthermore, the **value** of these gems is different, being even zero for some gems. In this updating of our problem, the episode ends when the agent enters in a cell ocuppied by a gem, regardless of its value.\n",
    "\n",
    "Our goal is to learn a policy which maximizes the long-term reward obtained by the agent, which is computed as the value associated to the collected gem minus the cost (negative rewards) associated to all the visited non-final states. \n",
    "\n",
    "Thus, the maze is a grid of size N x M formed by a set of cells, some of which can be occupied by walls, which cannot be crossed. The rest of the cells will be empty and they will represent  free space. For now, the robot can only move horizontally or vertically. In addition, we can have cells with gems, being the goal of our robot to collect the one of highest value. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a843aa",
   "metadata": {},
   "source": [
    "The objective of our robot is to collect the most valuable gem as fast as possible, in particular when $\\gamma$ is less than 1. Furthermore, the environment can be **noisy**, that is, the output of the proposed action may not be as expected. \n",
    "\n",
    "In order to implement our robot we have to take into account that: \n",
    "- The robot can start in a random cell of the map, but we will train our agent to solve a particular scenario.\n",
    "- The robot can move horizontally or vertically in the maze.\n",
    "- The robot cannot cross walls nor go beyond the bounds of the maze.\n",
    "- As the goal is achieved once the robot collect the first gem, it should avoid the cells containing less valuable gems. \n",
    "- The rewards for final and non-final states are provided by the environment.\n",
    "- On the contrary to previous assignments, our goal now is not to provide a path, but a policy for **all** the states in our environment (maze). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a54b239",
   "metadata": {},
   "source": [
    "##¬†Provided code\n",
    "\n",
    "In the following cells, we provide you some of the clases, implemented in `Python`, that will help you in the development of this assignment. \n",
    "\n",
    "First, we will import the necessary classes we need from the Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c390259",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29b8b25",
   "metadata": {},
   "source": [
    "Next, we will import some custom functions from the `utils.py` file. You don't need to modify those functions for the code to work, but feel free to have a look at them if you are curious. This code is identical to the one provided in Assignments 1 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f211045",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f102723",
   "metadata": {},
   "source": [
    "Finally, we will import some third party libraries. We will use those to display the problem in a graphical environment. In order to do that, we will use the magic functions from jupyter to install the library from inside the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec52733",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipythonblocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ace433f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ipythonblocks import BlockGrid, colors\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38d966c",
   "metadata": {},
   "source": [
    "In order to complete the requested policy-learning algorithms, in the following cells we provide you some fundamental classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c561cced",
   "metadata": {},
   "source": [
    "#### Class `Action`\n",
    "This class provides the **representation of the actions** that will be performed by the robot. You don't have to modify the code of this class. The possible actions will be: \"UP\", \"DOWN\", \"RIGHT\", \"LEFT\". This class is almost identical to the one provided in the previous assignments, but now there is no cost associated to any action (but to visiting states: rewards). To help us with policy visualization, we have added a new method that just returns a character which is a graphical representation of each action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59618256",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Action:\n",
    "    #actions = [\"UP\", \"DOWN\", \"RIGHT\", \"LEFT\"]\n",
    "\n",
    "    def __init__(self, move):\n",
    "        self.move = move\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"({self.move})\"\n",
    "    \n",
    "    def getChar(self):\n",
    "        if self.move == \"UP\":\n",
    "            return \"‚Üë\"\n",
    "        elif self.move == \"DOWN\":\n",
    "            return \"‚Üì\"\n",
    "        elif self.move == \"RIGHT\":\n",
    "            return \"‚Üí\"\n",
    "        elif self.move == \"LEFT\":\n",
    "            return \"‚Üê\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ce908c",
   "metadata": {},
   "source": [
    "#### Class `State`\n",
    "This class provides the **representation of a state** in the search space. In this problem, a state is defined in a simpler way than in the previous assignments. Now, the state is simply defined by the position of the agent. For coherence, we will use also **states** to represent the position of the gems. You don't have to modify the code of this class.\n",
    "\n",
    "Notice that the method `applyAction` is not needed anymore in this class. We will comment on that in the `Problem` class, but by now, try guessing the reason behind this üòâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6906843",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class State:\n",
    "\n",
    "    def __init__(self, pos):\n",
    "        self.pos = pos\n",
    "\n",
    "    # equals method. Returns true if the states are the same. \n",
    "    # Used for the hash table comparison, compares if both states are equal\n",
    "    def __eq__(self, state):\n",
    "        return self.pos == state.pos\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Position: {self.pos}\"\n",
    "\n",
    "    # hash method. Useful to index data structures that uses a hash function \n",
    "    #    to index elements, i.e. a Set()\n",
    "    def __hash__(self):\n",
    "        return int( math.pow(10,3)*self.pos[0] + self.pos[1])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca5bdfb",
   "metadata": {},
   "source": [
    "##¬†Implementation\n",
    "In the following cells, we provide you some classes and pieces of code that you will have to complete as a part of this assignment.\n",
    "\n",
    "\n",
    "#### Class `Problem`\n",
    "This class provides the **representation of the learning problem**. As you may notice, in this assignment, this class is a bit more complex than in the previous ones. The reason is that now we have to deal with an **uncertain environment**. \n",
    "\n",
    "It contains the size of the maze (`rows` and `cols`), the `initialState`, the `maze`, the `gems` positions, the `gemsRewards` and also the `moveProb` value, which codifies how uncertain is the environment. Thus, `moveProb=1.0` means a deterministic environment, but e.g. `moveProb=0.8` means that the selected action will be actually applied 80% of the times, while 20% of the times another possible action will be applied. You will have to implement some parts of this class. \n",
    "\n",
    "The class `Problem` also has some interesting methods:\n",
    "\n",
    "- `getPossibleActions(state)` will generate all the actions that can be applied in a given state. **You must code this method**. Notice that it is somehow similar to `getSucessors` in previous assignments. To make things easier we recommend programming this method to return a *pair of lists, where the first list will contain the possible actions, and the second list will contain the states reached with the actions from the first list*.\n",
    "\n",
    "- `getTransitionDistribution(state, action)` will return the transition probability distribution when trying to apply action `action` in state `state`, as a *list of triplets, where each triplet consists of a possible action, the state that action leads to, and the probability to land on that state*. Take a look to the code to understand its meaning.\n",
    "\n",
    "- `getReward(state)` returns *a number* representing the reward associated to the given state, regardless of whether it is a final state or not.\n",
    "\n",
    "- `applyAction(state, action)` returns the state obtained when we give the order to execute the action `action` in state `state`. However, because of the (probably) noisy environment, the result can be an unexpected outcome. This is the reason this method is now in the `Problem` class, because it needs to make use of the transition probabilities.\n",
    "\n",
    "- There are some other useful methods in this class, we encourage you to take a look at them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3cfa68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Problem:\n",
    "    actions = [\"UP\", \"DOWN\", \"RIGHT\", \"LEFT\"]\n",
    "\n",
    "    def __init__(self, rows, cols, seed, maxDivisions, gemsCount, transProbability, filename=\"\"):\n",
    "\n",
    "        if (filename != \"\"):\n",
    "            self.rows, self.cols, self.maze = readProblemInstance(filename)\n",
    "            print('Problem read with size', rows, 'x', cols)\n",
    "        else:\n",
    "            self.rows = rows\n",
    "            self.cols = cols\n",
    "            self.maze = getProblemInstance(rows, cols, maxDivisions, gemsCount, seed)\n",
    "\n",
    "        self.moveProb = transProbability  # probability of actually applying the desired action\n",
    "        self.gems = []\n",
    "        self.gemsRewards = []\n",
    "\n",
    "        for r in range(rows):\n",
    "            for c in range(cols):\n",
    "                if self.maze[r][c] == 2:\n",
    "                    init_point = (r, c)\n",
    "                elif self.maze[r][c] == 3:\n",
    "                    self.gems.append(State((r, c)))\n",
    "\n",
    "        self.initialState = State(init_point)\n",
    "\n",
    "        # setting rewards for final states.\n",
    "        # about 50% of the gems have no value\n",
    "\n",
    "        for gem in self.gems:\n",
    "            rw = 5*manhattan_distance(self.initialState.pos, gem.pos)\n",
    "            self.gemsRewards.append(int(rw * random.randint(0, 1)))\n",
    "\n",
    "    # returns the possible actions from a given state and the states achived by them\n",
    "\n",
    "    def getPossibleActions(self, state):\n",
    "\n",
    "        possibleActions = []\n",
    "        neighbours = []\n",
    "\n",
    "        #\n",
    "        # ¬†YOUR CODE HERE\n",
    "        #\n",
    "\n",
    "        UP = Action(\"UP\")\n",
    "        DOWN = Action(\"DOWN\")\n",
    "        RIGHT = Action(\"RIGHT\")\n",
    "        LEFT = Action(\"LEFT\")\n",
    "\n",
    "        if (state.pos[0] != 0):\n",
    "            if (self.maze[state.pos[0] - 1][state.pos[1]] != 1):\n",
    "                st = State((state.pos[0] - 1, state.pos[1]))\n",
    "                possibleActions.append(UP)\n",
    "                neighbours.append(st)\n",
    "\n",
    "        if (state.pos[0] != self.rows - 1):\n",
    "            if (self.maze[state.pos[0] + 1][state.pos[1]] != 1):\n",
    "                st = State((state.pos[0] + 1, state.pos[1]))\n",
    "                possibleActions.append(DOWN)\n",
    "                neighbours.append(st)\n",
    "\n",
    "        if (state.pos[1] != self.cols - 1):\n",
    "            if (self.maze[state.pos[0]][state.pos[1] + 1] != 1):\n",
    "                st = State((state.pos[0], state.pos[1] + 1))\n",
    "                possibleActions.append(RIGHT)\n",
    "                neighbours.append(st)\n",
    "\n",
    "        if (state.pos[1] != 0):\n",
    "            if (self.maze[state.pos[0]][state.pos[1] - 1] != 1):\n",
    "                st = State((state.pos[0], state.pos[1] - 1))\n",
    "                possibleActions.append(LEFT)\n",
    "                neighbours.append(st)\n",
    "\n",
    "        #\n",
    "        # END OF YOUR CODE\n",
    "        #\n",
    "\n",
    "        return possibleActions, neighbours\n",
    "\n",
    "    # returns the transition distribution from a given state when trying to apply an action\n",
    "    # for each possible action a tuple (action, resulting state, transition probability) is returned\n",
    "\n",
    "    def getTransitionDistribution(self, state, action):\n",
    "\n",
    "        distribution = []\n",
    "        posActions, neighbours = self.getPossibleActions(state)\n",
    "\n",
    "        if len(posActions) > 1:\n",
    "            noise = 1.0 - self.moveProb\n",
    "            for posAction, posNeighbour in zip(posActions, neighbours):\n",
    "                if posAction.move == action.move:\n",
    "                    distribution.append((posAction, posNeighbour, self.moveProb))\n",
    "                else:\n",
    "                    distribution.append((posAction, posNeighbour, noise / float(len(posActions) - 1)))\n",
    "        else:\n",
    "            distribution.append((posActions[0], neighbours[0], 1.0))\n",
    "\n",
    "        return distribution\n",
    "\n",
    "    # apply the given action (under possibly a noisy environment) a returns the obtained state\n",
    "\n",
    "    def applyAction(self, state, action):\n",
    "        distribution = self.getTransitionDistribution(state, action)\n",
    "\n",
    "        # we sample the actual action to be applied from the transition distribution probability\n",
    "\n",
    "        distWeights = [probability for action, state, probability in distribution]\n",
    "        selectedDistribution = random.choices(distribution, weights=distWeights)[0]\n",
    "        selectedState = copy.deepcopy(selectedDistribution[1])\n",
    "\n",
    "        return selectedState\n",
    "\n",
    "    # additional methods\n",
    "\n",
    "    def getInitialState(self):\n",
    "        return self.initialState\n",
    "\n",
    "    def getActions(self):\n",
    "        return self.actions\n",
    "\n",
    "    def isFinal(self, st):\n",
    "        '''\n",
    "        check if the given state is final or not\n",
    "        '''\n",
    "        return (st in self.gems)\n",
    "\n",
    "    def getGems(self):\n",
    "        return self.gems\n",
    "\n",
    "    def getGemsRewards(self):\n",
    "        return self.gemsRewards\n",
    "\n",
    "    def getReward(self, st):\n",
    "        try:\n",
    "            p = self.gems.index(st)\n",
    "        except:\n",
    "            p = -1\n",
    "\n",
    "        if p >= 0:\n",
    "            return self.gemsRewards[p]\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ccba2d",
   "metadata": {},
   "source": [
    "#### Class `ValueIteration`\n",
    "\n",
    "This class will contain the ValueIteration method as explained in the lectures. You will have to code most part of this class. **Hint: follow as much as possible the pseudocode of the method provided in the slides.**\n",
    "\n",
    "Remember that in this method we can exploit the knowledge about the environment (e.g. the states), the transition and reward models.\n",
    "\n",
    "Notice that a `dictionary` data structure is used to store both the policy (entries are pairs <state,action>) and the utilities (entries are pairs <state,value>).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696f94eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Value Iteration\n",
    "\n",
    "class ValueIteration:\n",
    "    def __init__(self, problem, gamma, threshold):\n",
    "        self.problem = problem\n",
    "        self.gamma = gamma\n",
    "        self.convergenceThreshold = threshold\n",
    "        self.policy = {}\n",
    "        self.utilities = {}\n",
    "        self.nfStates, self.fStates = self.getAllStates()\n",
    "        self.policy, self.utilities = self.initPolicyAndUtilities()\n",
    "\n",
    "    # check the maze and collect all positions non occupied by walls and not being final states (gems)\n",
    "    # Final states (gems) are returned on another list\n",
    "    def getAllStates(self):\n",
    "\n",
    "        nfStates = []  # non final states\n",
    "        fStates = []  # final states\n",
    "\n",
    "        for r in range(rows):\n",
    "            for c in range(cols):\n",
    "                if (self.problem.maze[r][c] != 1):\n",
    "                    if (self.problem.maze[r][c] != 3):\n",
    "                        nfStates.append(State((r, c)))\n",
    "                    else:\n",
    "                        fStates.append(State((r, c)))\n",
    "\n",
    "        return nfStates, fStates\n",
    "\n",
    "    # return the utilities for all the states.\n",
    "\n",
    "    def getUtilities(self):\n",
    "        return self.utilities\n",
    "\n",
    "    def initPolicyAndUtilities(self):\n",
    "        policy = {}\n",
    "        utilities = {}\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        # Initialize the utilities and policy dictionaries.\n",
    "\n",
    "        utilities = {st: 0 for st in self.nfStates}\n",
    "        policy = {st: random.choice(self.problem.actions) for st in self.nfStates}  # inicialmente se escoge una accion al azar por elegir alguna hasta que sea sustituida\n",
    "\n",
    "        for st in self.fStates:\n",
    "            policy[st]=random.choice(self.problem.actions)\n",
    "            utilities[st]=self.problem.getReward(st)\n",
    "\n",
    "        #\n",
    "        # END OF YOUR CODE\n",
    "        #\n",
    "        return policy, utilities\n",
    "\n",
    "    # main method for value iteration - learn the policy for all the non final states\n",
    "\n",
    "    def doLearning(self):\n",
    "\n",
    "        # init the utilities vector\n",
    "\n",
    "        utilities1 = {}\n",
    "        for st in self.fStates:\n",
    "            utilities1[st]=self.problem.getReward(st)\n",
    "\n",
    "        # Value Iteration main loop\n",
    "\n",
    "        delta = math.inf\n",
    "        it = 0\n",
    "        print(\"It: \", end='')\n",
    "\n",
    "        while (delta > self.convergenceThreshold):\n",
    "            it += 1\n",
    "            print(str(it) + \" \", end='')\n",
    "\n",
    "            delta = 0\n",
    "\n",
    "            #\n",
    "            # YOUR CODE HERE\n",
    "            # Program the value iteration loop.\n",
    "            # Hint: compute delta as the maximum difference between ut_i(state) and ut_{i+1}(state)\n",
    "            #\n",
    "\n",
    "            allStates = self.nfStates + self.fStates\n",
    "\n",
    "            for st in allStates:\n",
    "\n",
    "                if st in self.nfStates:\n",
    "                    maximo = 0\n",
    "                    numAccionesCalculadas = 0\n",
    "                    for accion in self.problem.getPossibleActions(st)[0]:\n",
    "                        x = self.problem.getTransitionDistribution(st, accion)\n",
    "                        suma = 0\n",
    "                        for i in x:\n",
    "                            suma = suma + (i[2] * self.utilities[i[1]])\n",
    "                        numAccionesCalculadas += 1\n",
    "\n",
    "                        if (suma > maximo) or (numAccionesCalculadas == 1):\n",
    "                            maximo = suma\n",
    "                            self.policy[st] = accion\n",
    "\n",
    "                    utilities1[st] = self.problem.getReward(st) + (self.gamma * maximo)\n",
    "\n",
    "                    if (abs(utilities1[st]-self.utilities[st]) > delta):\n",
    "                        delta = abs(utilities1[st]-self.utilities[st])\n",
    "                else:\n",
    "                    utilities1[st] = self.problem.getReward(st)\n",
    "\n",
    "            for st in allStates:\n",
    "                self.utilities[st] = utilities1[st]\n",
    "\n",
    "            #\n",
    "            # END OF YOUR CODE\n",
    "            #\n",
    "\n",
    "        return self.policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bdfdbe",
   "metadata": {},
   "source": [
    "#### Class `QTable`\n",
    "\n",
    "This class implements the QTable to be used by our QLearning algorithm.\n",
    "\n",
    "We have implemented it as a dictionary, where each entry is a pair <state,q_values>. We will have an entry for each state \"discovered/visited\" by our agent and the corresponding utility values (q_values) learned for every posible action in that state. The q_values are also implemented by a dictionary of pairs <action,value>. Remember that in QLearning the environment is discovered during the learning phase, thus, only visited states are included in the QTable.\n",
    "\n",
    "For your convenience, we provide all the methods needed to deal with this structure, so you don't have to modify this class, but if you feel more confortable by adding some additional method, please, feel free to do so. \n",
    "\n",
    "The provided methods are:\n",
    "\n",
    "- `contains(state)` returns `True` if there exists an entry for the given state in the QTable. \n",
    "\n",
    "- `addEntry(state, possibleActions)` adds a new entry for the given `state` and its possible actions. The q-value is set to 0 for all the actions.\n",
    "\n",
    "- `getValue(state, action)` returns the q-value for the pair <`state`,`action`>.\n",
    "\n",
    "- `modifyEntry(state, action, value)` replaces the q-value of the <`state`,`action`> pair with the new value `value`.\n",
    "\n",
    "- `getBestActon(state)` returns the action with maximum q-value for the given `state`.\n",
    "\n",
    "- `getRandomAction(state)` returns a randomly selected action for the given `state`.\n",
    "\n",
    "- `getOptimalPolicy()` returns a policy with the action of maximum q-value for each visited state (note that some states may not have been visited, so no action is provided for them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cda51a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# implemented as a dictionary with state as key and a dictionary with (action,q-value) as value\n",
    "\n",
    "class QTable:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.table = {}\n",
    "        \n",
    "    \n",
    "    # check if the state is already included in the q-table\n",
    "    \n",
    "    def contains(self, state):\n",
    "        return (state in self.table)\n",
    "       \n",
    "    \n",
    "    # add entry\n",
    "    \n",
    "    def addEntry(self, state, posActions):\n",
    "        qvalues = {}\n",
    "        for act in posActions:\n",
    "            qvalues[act.move] = 0.0\n",
    "            \n",
    "        self.table[state] = qvalues\n",
    "\n",
    "    # get q-value\n",
    "    \n",
    "    def getValue(self, state, action):\n",
    "        qvalues = self.table[state]\n",
    "        return qvalues[action.move]\n",
    "    \n",
    "    # modify entry \n",
    "    \n",
    "    def modifyEntry(self, state, action, value):\n",
    "        qvalues = self.table[state]\n",
    "        qvalues[action.move] = value\n",
    "        \n",
    "    # get best action for a given state\n",
    "    \n",
    "    def getBestAction(self, state):\n",
    "        qvalues = self.table[state]\n",
    "        bestQ = -math.inf\n",
    "        bestAct = None\n",
    "        for act, q in qvalues.items():\n",
    "            if q > bestQ:\n",
    "                bestQ = q\n",
    "                bestAct = act\n",
    "        \n",
    "        return Action(bestAct)\n",
    "    \n",
    "    # get a random action for a given state\n",
    "    \n",
    "    def getRandomAction(self, state):\n",
    "        qvalues = self.table[state]\n",
    "        act, q = random.choice(list(qvalues.items()))\n",
    "        \n",
    "        return Action(act)\n",
    "        \n",
    "   \n",
    "    # get policy from q-table\n",
    "    \n",
    "    def getOptimalPolicy(self):\n",
    "        policy = {}\n",
    "        \n",
    "        for state, qvalues in self.table.items():\n",
    "            action = self.getBestAction(state)\n",
    "            policy[state] = action\n",
    "        \n",
    "        return policy\n",
    "    \n",
    "    # print the q-table\n",
    "    \n",
    "    def printQTable(self):\n",
    "        for state, qvalues in self.table.items():\n",
    "            print(f'{state}: ', end='')\n",
    "            for action, value in qvalues.items():\n",
    "                print(f'{action} {value} ', end='')\n",
    "            print()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33623de4",
   "metadata": {},
   "source": [
    "#### Class `QLearning`\n",
    "\n",
    "This class will contain the Q-Learning method as explained in the lectures. You will have to code most part of this class. **Hint: follow as much as possible the pseudocode of the algorithm provided in the slides of unit 5.**\n",
    "\n",
    "Remember that in this method we **cannot** exploit the knowledge about the environment (e.g. the states), the transition and reward models. We must discover the states during learning and ask the environment (`Problem` class) in order to know the result (state and reward) when applying an action in a given state.\n",
    "\n",
    "Notice that a dictionary data structure is used to store the `policy` (entries are pairs <state,action>).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5513a13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Q-Learning: Reinforcement Learning by temporal difference\n",
    "\n",
    "class QLearning:\n",
    "    \n",
    "    def __init__(self, problem, gamma, alpha, exploration, episodes):\n",
    "        self.problem = problem\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.exploration = exploration\n",
    "        self.numEpisodes = episodes\n",
    "        \n",
    "        self.policy = {}\n",
    "        \n",
    "        self.qTable = QTable()\n",
    "        self.initialState = self.problem.getInitialState()\n",
    "        posActions, others = self.problem.getPossibleActions(self.initialState)\n",
    "        self.qTable.addEntry(self.initialState, posActions)\n",
    "        \n",
    "    \n",
    "    #¬†main method for Q-learning - learn the policy\n",
    "    \n",
    "    def doLearning(self):\n",
    "\n",
    "        for e in range(self.numEpisodes):\n",
    "\n",
    "            state = self.initialState\n",
    "            nuevoEstado = self.initialState\n",
    "\n",
    "            while not self.problem.isFinal(state):\n",
    "\n",
    "                #\n",
    "                # YOUR CODE HERE\n",
    "                # Implement Q-Learning\n",
    "                #\n",
    "                \n",
    "                if random.random() < exploration:\n",
    "                    action = self.qTable.getRandomAction(state)\n",
    "                else:\n",
    "                    action = self.qTable.getBestAction(state)\n",
    "                    \n",
    "                nuevoEstado = self.problem.applyAction(state,action)\n",
    "                recompensa = self.problem.getReward(nuevoEstado)\n",
    "\n",
    "                if not self.qTable.contains(nuevoEstado):\n",
    "                    self.qTable.addEntry(nuevoEstado,self.problem.getPossibleActions(nuevoEstado)[0])\n",
    "\n",
    "                valor = (1-self.alpha) * self.qTable.getValue(state, action) + self.alpha * (recompensa + (self.gamma * self.qTable.getValue(nuevoEstado,self.qTable.getBestAction(nuevoEstado))))\n",
    "                self.qTable.modifyEntry(state, action, valor)\n",
    "                state = nuevoEstado\n",
    "\n",
    "                #\n",
    "                # END OF YOUR CODE\n",
    "                #\n",
    "\n",
    "        # compute optimal policy\n",
    "\n",
    "        self.policy = self.qTable.getOptimalPolicy()\n",
    "        return self.policy\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1197db2a",
   "metadata": {},
   "source": [
    "The following method prints (in text) a given policy over the problem maze to be solved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4d2801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae1726b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prints the policy over the maze\n",
    "\n",
    "def printPolicy(problem, policy):\n",
    "    rows = problem.rows\n",
    "    cols = problem.cols\n",
    "    maze = problem.maze\n",
    "    \n",
    "    print()\n",
    "    for row in range(rows):\n",
    "        print()\n",
    "        for col in range(cols):\n",
    "            if ((maze[row][col] == 1)):\n",
    "                print(\"‚ñ†\" + \" \", end='')\n",
    "            elif ((maze[row][col] == 3)):\n",
    "                print(\"*\" + \" \", end='')\n",
    "            else:\n",
    "                state = State((row,col))\n",
    "                if state in policy:\n",
    "                    act = policy[state]\n",
    "                    print(act.getChar() + \" \", end='')\n",
    "                else:\n",
    "                    print(\"_ \", end='')\n",
    "    print()\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490d6dc1",
   "metadata": {},
   "source": [
    "The following methods run a given policy a number of times and return the obtained (averaged) reward (utility). They are useful to evaluate the learnt policy. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8481d285",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# runs an episode for the given policy. Returns the obtained utility\n",
    "\n",
    "def singlePolicyRun(problem, policy, gamma):\n",
    "   \n",
    "    utility = 0.0\n",
    "   \n",
    "    state = problem.getInitialState()\n",
    "\n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    #\n",
    "\n",
    "    while not problem.isFinal(state):\n",
    "        state = problem.applyAction(state, policy[state])\n",
    "        utility = utility + gamma * problem.getReward(state)\n",
    "    \n",
    "    # \n",
    "    # END OF YOU CODE\n",
    "    #\n",
    "    \n",
    "    return utility\n",
    "\n",
    "\n",
    "# runs a series of episodes for the given policy. Returns the averaged utility.\n",
    "\n",
    "def multiplePolicyRun(problem, policy, gamma, numIterations):\n",
    "    \n",
    "    meanUtility = 0.0\n",
    "    \n",
    "    for it in range(numIterations):\n",
    "        meanUtility += singlePolicyRun(problem, policy, gamma) \n",
    "                \n",
    "    meanUtility /= numIterations\n",
    "    \n",
    "    return meanUtility \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b474de0b",
   "metadata": {},
   "source": [
    "#### The `main` function\n",
    "\n",
    "Next, we provide you the `main` function that creates the problem and solves it using the policy learning algorithms. This method should be used afterwards to carry out the experimentation to study the behaviour of the implemented algorithms for different values of the parameters provided (size of the maze, maximum number of walls, number of gem cells, and algorithm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06105bf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main(rows, cols, seed, maxDivisions, gemsCount, transitionProb, algorithm, gamma, convThreshold, alpha, exploration, numEpisodes):\n",
    "    problem = Problem(rows, cols, seed, maxDivisions, gemsCount, transitionProb)\n",
    "\n",
    "    print(\"$ python assignment3 \" + str(problem.rows) + \" \" + str(problem.cols) + \" \" + str(seed) + \" \" \n",
    "          + str(maxDivisions) + \" \" + str(gemsCount) + \" \" +  str(transitionProb) + \" \" + \n",
    "          algorithm + \" \" + str(gamma) + \" \" + str(convThreshold) + \" \" + str(alpha) + \" \" +\n",
    "          str(exploration) + \" \" + str(numEpisodes) + \"\\n\")\n",
    "\n",
    "    print(\"Problem instance:\")\n",
    "    printMaze(problem.maze)\n",
    "    print(\"\")\n",
    "    \n",
    "    # print gems values\n",
    "    gems = problem.getGems()\n",
    "    rewards = problem.getGemsRewards()\n",
    "    \n",
    "    for i in range(len(gems)):\n",
    "        print(\"Gem at \" + str(gems[i]) + \" has a reward of: \" + str(rewards[i]))\n",
    "\n",
    "    learner = None\n",
    "\n",
    "    if algorithm == \"ValueIteration\":\n",
    "        learner = ValueIteration(problem,gamma,convThreshold)\n",
    "    elif algorithm == \"QLearning\":\n",
    "        learner = QLearning(problem, gamma, alpha, exploration, numEpisodes)\n",
    "    else:\n",
    "        raise Exception\n",
    "\n",
    "    time_start = time.perf_counter()\n",
    "    policy = learner.doLearning()\n",
    "    time_end = time.perf_counter()\n",
    "    print(\"\")\n",
    "    print(\"Elapsed time: \" + str(time_end - time_start) + \" seconds\")\n",
    "\n",
    "    return policy, problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef27f4ef",
   "metadata": {},
   "source": [
    "Below an example of scenario to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f8671d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter List \n",
    "\n",
    "rows = 10\n",
    "cols = 10\n",
    "seed = 2020\n",
    "maxDivisions = 2\n",
    "gemsCount = 5\n",
    "transitionProb = 1.0 # deterministic, use e.g. 0.8 for a noisy environment\n",
    "\n",
    "gamma = 1.0\n",
    "threshold = 0.001\n",
    "alpha = 0.8\n",
    "exploration = 0 # no exploration is considered, the agent always behave greedily during learning\n",
    "numEpisodes = 10000  # for learning\n",
    "numIterations = 100  # for evaluation\n",
    "\n",
    "algorithm = \"ValueIteration\"\n",
    "#algorithm = \"QLearning\"\n",
    "\n",
    "policy, problem_instance = main(rows, cols, seed, maxDivisions, gemsCount, transitionProb, algorithm, gamma, threshold, alpha, exploration, numEpisodes)\n",
    "\n",
    "printPolicy(problem_instance, policy)\n",
    "\n",
    "ut = multiplePolicyRun(problem_instance, policy, gamma, numIterations)\n",
    "\n",
    "print(\"\\nObtained utility (over \" + str(numIterations) + \" iterations) for learnt policy is: \" + str(ut))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1270f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter List Pregunta 1\n",
    "\n",
    "rows = 12\n",
    "cols = 14\n",
    "seed = 952\n",
    "maxDivisions = 8\n",
    "gemsCount = 7\n",
    "transitionProb = 0.8\n",
    "numIterations = 100\n",
    "gamma = 1.0\n",
    "threshold = 0.001\n",
    "\n",
    "algorithm = \"ValueIteration\"\n",
    "#algorithm = \"QLearning\"\n",
    "\n",
    "policy, problem_instance = main(rows, cols, seed, maxDivisions, gemsCount, transitionProb, algorithm, gamma, threshold, alpha, exploration, numEpisodes)\n",
    "\n",
    "printPolicy(problem_instance, policy)\n",
    "\n",
    "ut = multiplePolicyRun(problem_instance, policy, gamma, numIterations)\n",
    "\n",
    "print(\"\\nObtained utility (over \" + str(numIterations) + \" iterations) for learnt policy is: \" + str(ut))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2577a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter List Pregunta 2\n",
    "\n",
    "rows = 12\n",
    "cols = 14\n",
    "seed = 952\n",
    "maxDivisions = 8\n",
    "gemsCount = 7\n",
    "transitionProb = 0.8\n",
    "numIterations = 100\n",
    "gamma = 1.0\n",
    "threshold = 0.001\n",
    "alpha = 0.9\n",
    "exploration = 0\n",
    "numEpisodes = 1000\n",
    "\n",
    "#algorithm = \"ValueIteration\"\n",
    "algorithm = \"QLearning\"\n",
    "\n",
    "policy, problem_instance = main(rows, cols, seed, maxDivisions, gemsCount, transitionProb, algorithm, gamma, threshold, alpha, exploration, numEpisodes)\n",
    "\n",
    "printPolicy(problem_instance, policy)\n",
    "\n",
    "ut = multiplePolicyRun(problem_instance, policy, gamma, numIterations)\n",
    "\n",
    "print(\"\\nObtained utility (over \" + str(numIterations) + \" iterations) for learnt policy is: \" + str(ut))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8b1259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter List Pregunta 3\n",
    "\n",
    "#Fijos:\n",
    "\n",
    "rows = 12\n",
    "cols = 14\n",
    "seed = 952\n",
    "maxDivisions = 8\n",
    "gemsCount = 7\n",
    "transitionProb = 0.8\n",
    "numIterations = 100\n",
    "gamma = 1.0\n",
    "\n",
    "#los que se pueden cambiar:\n",
    "\n",
    "threshold = 0.001\n",
    "alpha = 0.8\n",
    "exploration = 0\n",
    "numEpisodes = 1000\n",
    "\n",
    "#algorithm = \"ValueIteration\"\n",
    "algorithm = \"QLearning\"\n",
    "\n",
    "policy, problem_instance = main(rows, cols, seed, maxDivisions, gemsCount, transitionProb, algorithm, gamma, threshold, alpha, exploration, numEpisodes)\n",
    "\n",
    "printPolicy(problem_instance, policy)\n",
    "\n",
    "ut = multiplePolicyRun(problem_instance, policy, gamma, numIterations)\n",
    "\n",
    "print(\"\\nObtained utility (over \" + str(numIterations) + \" iterations) for learnt policy is: \" + str(ut))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c28c11d",
   "metadata": {},
   "source": [
    "#### Printing the result\n",
    "\n",
    "Here we provide you some code to display the maze and the path carried out by the robot (the green cell) to solve the instance of the problem. Walls are represented as black cells, and garbage with brown cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5500107",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def render_maze(grid, maze, gems, gemsRewards):\n",
    "    height, width = len(maze), len(maze[0])\n",
    "    \n",
    "    # Render maze\n",
    "    for i in range(width):\n",
    "        for j in range(height):\n",
    "            grid[j,i] = colors.LightGray if maze[j][i] in [0,2,3] else colors.Black\n",
    "\n",
    "            \n",
    "    # Render gems in maze --> brightness based on reward    \n",
    "    purpleMaxDif = [a - b for a, b in zip(colors.Violet, colors.Purple)]    \n",
    "    maxReward = max(gemsRewards)\n",
    "    minReward = min(gemsRewards)\n",
    "    RewardRange = maxReward - minReward\n",
    "    for gem, reward in zip(gems, gemsRewards):\n",
    "        weight = float(reward - minReward) / float(RewardRange)\n",
    "        grid[gem.pos] = [int(a + b*weight) for a,b in zip(colors.Purple, purpleMaxDif)]\n",
    "        \n",
    "def find_agent(maze, width, height):\n",
    "    for i in range(width):\n",
    "        for j in range(height):\n",
    "            if (maze[j][i] == 2):\n",
    "                return (i,j)\n",
    "\n",
    "def render_problem(policy, problem_instance, gamma = 1.0):\n",
    "    height, width = len(problem_instance.maze), len(problem_instance.maze[0])\n",
    "    solution_grid = BlockGrid(width, height, fill=colors.LightGray, lines_on=True)\n",
    "    agentPos = find_agent(problem_instance.maze, width, height)\n",
    "    \n",
    "    # Initial position rendering\n",
    "    render_maze(solution_grid, problem_instance.maze, problem_instance.gems, problem_instance.gemsRewards)\n",
    "    solution_grid[agentPos[1],agentPos[0]] = colors.Green\n",
    "    solution_grid.show()\n",
    "    \n",
    "    agent_state = State((agentPos[1], agentPos[0]))\n",
    "    \n",
    "    g = gamma\n",
    "    numSteps = 0\n",
    "    reward = 0\n",
    "        \n",
    "    while (agent_state not in problem_instance.gems):\n",
    "        agent_state = problem_instance.applyAction(agent_state, policy[agent_state])\n",
    "        \n",
    "        reward += g*problem_instance.getReward(agent_state)\n",
    "        g *= g\n",
    "        numSteps += 1\n",
    "        \n",
    "        # Render maze\n",
    "        render_maze(solution_grid, problem_instance.maze, problem_instance.gems, problem_instance.gemsRewards)\n",
    "        \n",
    "        # Render agent, and update its position and the garbage list\n",
    "        solution_grid[agent_state.pos[0], agent_state.pos[1]] = colors.Green\n",
    "        \n",
    "        solution_grid.show()\n",
    "        clear_output(wait=True)\n",
    "        time.sleep(0.2)\n",
    "        \n",
    "    print(f'Obtained utility after {numSteps} steps and reaching gem with reward {problem_instance.getReward(agent_state)}: {reward}')\n",
    "    solution_grid.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e90ddbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "render_problem(policy, problem_instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9533326",
   "metadata": {},
   "source": [
    "## Experimental results\n",
    "\n",
    "Once the algorithms have been implemented, you must study their performance. In order to do that, you must compare the quality of the solutions obtained, as well as the number of expanded nodes for instances of different maze sizes, number of walls and number of garbage cells.\n",
    "\n",
    "Please, use new cells to insert code to carry out the experimental results and study of the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b35c38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
